# GPT-2 åŒ»ç–—å¥åº·å’¨è¯¢èŠå¤©æœºå™¨äºº

åŸºäº GPT-2 æ¨¡å‹æ„å»ºçš„åŒ»ç–—å¥åº·å’¨è¯¢èŠå¤©æœºå™¨äººï¼Œä½¿ç”¨ Flask æä¾› Web ç•Œé¢ã€‚æœ¬é¡¹ç›®å®ç°äº†ä»æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒåˆ°éƒ¨ç½²çš„å®Œæ•´æµç¨‹ï¼Œæ”¯æŒå¤šè½®å¯¹è¯å’Œä¸Šä¸‹æ–‡ç†è§£ã€‚

- **æ•°æ®å¤„ç†**ï¼šæ ¼å¼è½¬æ¢ã€å¼ é‡è½¬æ¢ï¼Œå°è£… DataSet ä¸ DataLoader å¯¹è±¡ï¼Œé€‚é…æ¨¡å‹è¾“å…¥è§„èŒƒï¼›
- **è®¾è®¡æ¨¡å‹è®­ç»ƒç­–ç•¥**ï¼šå®Œæˆ Train / Validate å…¨æµç¨‹ï¼Œä½¿ç”¨ Top-P å‘æ•£å¼ç”Ÿæˆã€å¯¹è¯ historyã€æƒ©ç½šç³»æ•°ã€Warmup å­¦ä¹ ç‡è°ƒèŠ‚ç­‰ï¼›
- **äººæœºäº¤äº’å®ç°**ï¼šå¼€å‘æ¨¡å‹é¢„æµ‹æ¨¡å—ï¼Œä½¿ç”¨ Flask æ¡†æ¶å¼€å‘ API æ¥å£ï¼Œå®ç°æœºå™¨äººä¸Šçº¿åº”ç”¨ã€‚

## ğŸ”„ é¡¹ç›®å®ç°æµç¨‹ä¸æ ¸å¿ƒä»£ç ç‰‡æ®µ

### 1. æ•°æ®é¢„å¤„ç†ä¸æ•°æ®é›†æ„å»º

å°†åŸå§‹å¤šè½®å¯¹è¯æ–‡æœ¬è½¬ä¸ºæ¨¡å‹å¯ç”¨çš„ ID åºåˆ—ï¼Œå¹¶ä¿å­˜ä¸º `pkl` æ–‡ä»¶ï¼ˆèŠ‚é€‰è‡ª `data_preprocess/preprocess.py`ï¼‰ï¼š

```python
tokenizer = BertTokenizer(
    '../vocab/vocab.txt',
    sep_token='[SEP]',
    pad_token='[PAD]',
    cls_token='[CLS]',
)

with open(train_txt_path, 'rb') as f:
    data = f.read().decode('utf-8')

if '\r\n' in data:
    train_data = data.split('\r\n\r\n')
else:
    train_data = data.split('\n\n')

dialogue_list = []
for dialogue in train_data:
    sequences = dialogue.split('\r\n') if '\r\n' in dialogue else dialogue.split('\n')
    input_ids = [tokenizer.cls_token_id]
    for sequence in sequences:
        input_ids += tokenizer.encode(sequence, add_special_tokens=False)
        input_ids.append(tokenizer.sep_token_id)
    dialogue_list.append(input_ids)

with open(train_pkl_path, 'wb') as f:
    pickle.dump(dialogue_list, f)
```

ä¹‹åé€šè¿‡ `dataset.py` / `dataloader.py` å°† `pkl` æ•°æ®å°è£…ä¸º `Dataset` å’Œ `DataLoader`ï¼Œå®Œæˆå¼ é‡åŒ–å’Œæ‰¹å¤„ç†ã€‚

### 2. æ¨¡å‹è®­ç»ƒä¸éªŒè¯æµç¨‹

è®­ç»ƒæµç¨‹åŒ…æ‹¬å•ä¸ª epoch çš„å‰å‘ã€åå‘ä¸æ¢¯åº¦æ›´æ–°ï¼Œä»¥åŠåŸºäºéªŒè¯é›†çš„æœ€ä¼˜æ¨¡å‹ä¿å­˜ï¼ˆèŠ‚é€‰è‡ª `train.py`ï¼‰ï¼š

```python
def train_epoch(model, train_dataloader, optimizer, scheduler, epoch, args):
    model.train()
    device = args.device
    ignore_index = args.ignore_index
    total_loss = 0

    for batch_idx, (input_ids, labels) in enumerate(train_dataloader):
        input_ids = input_ids.to(device)
        labels = labels.to(device)

        outputs = model(input_ids, labels=labels)
        logits = outputs.logits
        loss = outputs.loss.mean()

        batch_correct_num, batch_total_num = calculate_acc(logits, labels, ignore_index=ignore_index)
        if args.gradient_accumulation_steps > 1:
            loss = loss / args.gradient_accumulation_steps

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)

        if (batch_idx + 1) % args.gradient_accumulation_steps == 0:
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
```

è®­ç»ƒä¸»å‡½æ•°è´Ÿè´£ï¼šåˆå§‹åŒ–é…ç½®ã€æ„å»º tokenizer ä¸ GPT-2 æ¨¡å‹ã€åŠ è½½æ•°æ®ã€è°ƒç”¨è®­ç»ƒä¸éªŒè¯ï¼Œå¹¶åŸºäºéªŒè¯é›†æŸå¤±ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼š

```python
params = ParameterConfig()
tokenizer = BertTokenizerFast(
    params.vocab_path,
    sep_token='[SEP]',
    pad_token='[PAD]',
    cls_token='[CLS]',
)

if params.pretrained_model:
    model = GPT2LMHeadModel.from_pretrained(params.pretrained_model)
else:
    model_config = GPT2Config.from_pretrained(params.config_json)
    model = GPT2LMHeadModel(config=model_config)
model = model.to(params.device)

train_dataloader, validate_dataloader = get_dataloader(params.valid_path, params.valid_path)
train(model, train_dataloader, validate_dataloader, params)
```

è®­ç»ƒä¸­ä½¿ç”¨ **å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰** å’Œ **æ¢¯åº¦è£å‰ª** ç­‰ç­–ç•¥ï¼Œæå‡æ”¶æ•›ç¨³å®šæ€§ã€‚

### 3. å¯¹è¯ç”Ÿæˆç­–ç•¥ï¼ˆTop-K / Top-P + é‡å¤æƒ©ç½šï¼‰

æ¨ç†é˜¶æ®µé€šè¿‡ Top-K + Top-P é‡‡æ ·é…åˆé‡å¤æƒ©ç½šï¼Œæ§åˆ¶ç”Ÿæˆå¤šæ ·æ€§ä¸ç¨³å®šæ€§ï¼ˆèŠ‚é€‰è‡ª `flask_predict.py`ï¼‰ï¼š

```python
def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
    assert logits.dim() == 1
    top_k = min(top_k, logits.size(-1))

    if top_k > 0:
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = filter_value

    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[indices_to_remove] = filter_value
    return logits
```

ç»“åˆå¯¹è¯å†å²ä¸é‡‡æ ·ç­–ç•¥çš„ç”Ÿæˆå‡½æ•°ï¼ˆèŠ‚é€‰ï¼‰ï¼š

```python
def model_predict(text):
    history = []
    text_ids = tokenizer.encode(text, add_special_tokens=False)
    history.append(text_ids)

    input_ids = [tokenizer.cls_token_id]
    for history_utr in history[-pconf.max_history_len:]:
        input_ids.extend(history_utr)
        input_ids.append(tokenizer.sep_token_id)

    input_ids = torch.tensor(input_ids).long().to(device).unsqueeze(0)
    response = []

    for _ in range(pconf.max_len):
        outputs = model(input_ids=input_ids)
        logits = outputs.logits
        next_token_logits = logits[0, -1, :]

        for id in set(response):
            next_token_logits[id] /= pconf.repetition_penalty
        next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')

        filtered_logits = top_k_top_p_filtering(
            next_token_logits, top_k=pconf.topk, top_p=pconf.topp
        )
        next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)
        if next_token == tokenizer.sep_token_id:
            break

        response.append(next_token.item())
        input_ids = torch.cat((input_ids, next_token.unsqueeze(0)), dim=1)

    text = tokenizer.convert_ids_to_tokens(response)
    return ''.join(text)
```

### 4. äººæœºäº¤äº’ä¸éƒ¨ç½²ï¼ˆFlask Webï¼‰

é€šè¿‡ Flask æä¾› Web ç•Œé¢ï¼Œå®ç°å‰ç«¯è¡¨å•è¾“å…¥ä¸æ¨¡å‹åç«¯æ¨ç†çš„æ‰“é€šï¼ˆèŠ‚é€‰è‡ª `app.py`ï¼‰ï¼š

```python
from flask import Flask, render_template, request
from flask_predict import model_predict

app = Flask(__name__)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/ask', methods=['POST'])
def ask():
    user_input = request.form['user_input']
    response = model_predict(user_input)
    return render_template('index.html', user_input=user_input, answer=response)

if __name__ == '__main__':
    app.run(debug=True)
```

è‡³æ­¤å½¢æˆå®Œæ•´é—­ç¯ï¼š**åŸå§‹å¯¹è¯æ•°æ® â†’ é¢„å¤„ç†ä¸æ•°æ®åŠ è½½ â†’ GPT-2 è®­ç»ƒä¸éªŒè¯ â†’ æ¨¡å‹éƒ¨ç½²ä¸ Web äº¤äº’**ã€‚

## âœ¨ åŠŸèƒ½ç‰¹ç‚¹

- ğŸ¤– **åŸºäº GPT-2 æ¨¡å‹**ï¼šä½¿ç”¨ GPT-2 æ¶æ„è¿›è¡Œå¯¹è¯ç”Ÿæˆ
- ğŸ’¬ **å¤šè½®å¯¹è¯æ”¯æŒ**ï¼šæ”¯æŒå†å²å¯¹è¯ä¸Šä¸‹æ–‡ç†è§£ï¼ˆé»˜è®¤ä¿ç•™æœ€è¿‘ 3 è½®ï¼‰
- ğŸ¥ **åŒ»ç–—å¥åº·åœºæ™¯**ï¼šä¸“æ³¨äºåŒ»ç–—å¥åº·å’¨è¯¢é¢†åŸŸçš„å¯¹è¯ç”Ÿæˆ
- ğŸŒ **Web ç•Œé¢**ï¼šæä¾›å‹å¥½çš„ Flask Web äº¤äº’ç•Œé¢
- ğŸ“± **å‘½ä»¤è¡Œäº¤äº’**ï¼šæ”¯æŒå‘½ä»¤è¡Œæ¨¡å¼è¿›è¡Œå¿«é€Ÿæµ‹è¯•
- ğŸ¯ **æ™ºèƒ½é‡‡æ ·**ï¼šä½¿ç”¨ Top-K å’Œ Top-P (Nucleus) é‡‡æ ·ç­–ç•¥
- ğŸ”„ **é‡å¤æƒ©ç½š**ï¼šå†…ç½®é‡å¤æƒ©ç½šæœºåˆ¶ï¼Œå‡å°‘é‡å¤ç”Ÿæˆ
- ğŸš€ **GPU åŠ é€Ÿ**ï¼šæ”¯æŒ CUDA åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

- **Python**: 3.7+
- **PyTorch**: 1.8+ï¼ˆæ¨èä½¿ç”¨ CUDA ç‰ˆæœ¬ä»¥æ”¯æŒ GPU åŠ é€Ÿï¼‰
- **CUDA**: 11.0+ï¼ˆå¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿï¼‰
- **å†…å­˜**: å»ºè®® 8GB+ RAM
- **æ˜¾å­˜**: å¦‚æœä½¿ç”¨ GPUï¼Œå»ºè®® 4GB+ VRAM

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å…‹éš†é¡¹ç›®

```bash
git clone <your-repo-url>
cd GPT2_Chatbot
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

**æ³¨æ„**ï¼šå¦‚æœéœ€è¦ä½¿ç”¨ GPU åŠ é€Ÿï¼Œè¯·å®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorchï¼š

- è®¿é—® [PyTorch å®˜ç½‘](https://pytorch.org/get-started/previous-versions/) æŸ¥çœ‹å¯¹åº”ç‰ˆæœ¬çš„å®‰è£…å‘½ä»¤
- æˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼ˆæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬è°ƒæ•´ï¼Œä»¥ CUDA 11.8 ä¸ºä¾‹ï¼‰ï¼š
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 3. ä¸‹è½½æ¨¡å‹æ–‡ä»¶

ç”±äºæ¨¡å‹æ–‡ä»¶è¾ƒå¤§ï¼Œæ— æ³•ç›´æ¥åŒ…å«åœ¨ Git ä»“åº“ä¸­ã€‚ä½ éœ€è¦ï¼š

1. **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**ï¼š
   - å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶æ”¾ç½®åœ¨ `save_model/epoch97/` ç›®å½•ä¸‹ ï¼ˆæˆ–è‡ªå®šä¹‰ç›®å½•æ­¤epoch97åªæ˜¯è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
   - æ¨¡å‹ç›®å½•åº”åŒ…å«ï¼š
     - `config.json` - æ¨¡å‹é…ç½®æ–‡ä»¶
     - `pytorch_model.bin` æˆ– `model.safetensors` - æ¨¡å‹æƒé‡æ–‡ä»¶

2. **æ¨¡å‹æ–‡ä»¶ç»“æ„**ï¼š
```
save_model/
â””â”€â”€ epoch97/
    â”œâ”€â”€ config.json
    â””â”€â”€ pytorch_model.bin  # æˆ– model.safetensors
```

**æç¤º**ï¼šå¦‚æœæ¨¡å‹æ–‡ä»¶åœ¨å…¶ä»–ä½ç½®ï¼Œå¯ä»¥ï¼š
- ä½¿ç”¨ Git LFS ä¸Šä¼ å¤§æ–‡ä»¶
- ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆå¦‚ Google Driveã€ç™¾åº¦ç½‘ç›˜ç­‰ï¼‰å¹¶æä¾›ä¸‹è½½é“¾æ¥
- åœ¨ Releases ä¸­æä¾›æ¨¡å‹æ–‡ä»¶ä¸‹è½½

### 4. éªŒè¯æ–‡ä»¶ç»“æ„

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š
- âœ… `vocab/vocab.txt` - è¯æ±‡è¡¨æ–‡ä»¶ï¼ˆå·²åŒ…å«åœ¨ä»“åº“ä¸­ï¼‰
- âœ… `config/config.json` - æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆå·²åŒ…å«åœ¨ä»“åº“ä¸­ï¼‰
- âœ… `save_model/epoch97/` - æ¨¡å‹æ–‡ä»¶ç›®å½•ï¼ˆéœ€è¦ä¸‹è½½ï¼‰

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### Web ç•Œé¢è¿è¡Œ

å¯åŠ¨ Flask Web åº”ç”¨ï¼š

```bash
python app.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:5000`

**æ³¨æ„**ï¼šé»˜è®¤è¿è¡Œåœ¨è°ƒè¯•æ¨¡å¼ï¼ˆ`debug=True`ï¼‰ï¼Œç”Ÿäº§ç¯å¢ƒè¯·ä¿®æ”¹ `app.py` ä¸­çš„é…ç½®ã€‚

### å‘½ä»¤è¡Œäº¤äº’è¿è¡Œ

ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼è¿›è¡Œäº¤äº’ï¼š

```bash
python interact.py
```

è¾“å…¥ `quit` æˆ– `exit` é€€å‡ºç¨‹åºã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
GPT2_Chatbot/
â”œâ”€â”€ app.py                 # Flask Web åº”ç”¨å…¥å£
â”œâ”€â”€ flask_predict.py       # Flask é¢„æµ‹æ¨¡å—ï¼ˆæ¨¡å‹åŠ è½½å’Œæ¨ç†ï¼‰
â”œâ”€â”€ interact.py            # å‘½ä»¤è¡Œäº¤äº’è„šæœ¬
â”œâ”€â”€ train.py              # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ parameter_config.py   # å‚æ•°é…ç½®æ–‡ä»¶
â”œâ”€â”€ function_tools.py     # å·¥å…·å‡½æ•°ï¼ˆæŸå¤±è®¡ç®—ã€å‡†ç¡®ç‡è®¡ç®—ç­‰ï¼‰
â”œâ”€â”€ requirements.txt      # Python ä¾èµ–
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.json       # GPT-2 æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ vocab/
â”‚   â”œâ”€â”€ vocab.txt         # è¯æ±‡è¡¨æ–‡ä»¶
â”‚   â””â”€â”€ vocab2.txt        # å¤‡ç”¨è¯æ±‡è¡¨
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html        # Web ç•Œé¢æ¨¡æ¿
â”‚   â””â”€â”€ index1.html       # å¤‡ç”¨ç•Œé¢æ¨¡æ¿
â”œâ”€â”€ data/                 # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ medical_train.pkl # è®­ç»ƒæ•°æ®ï¼ˆé¢„å¤„ç†åï¼‰
â”‚   â”œâ”€â”€ medical_train.txt # è®­ç»ƒæ•°æ®ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
â”‚   â”œâ”€â”€ medical_valid.pkl # éªŒè¯æ•°æ®ï¼ˆé¢„å¤„ç†åï¼‰
â”‚   â””â”€â”€ medical_valid.txt # éªŒè¯æ•°æ®ï¼ˆåŸå§‹æ–‡æœ¬ï¼‰
â”œâ”€â”€ data_preprocess/      # æ•°æ®é¢„å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ dataloader.py     # æ•°æ®åŠ è½½å™¨
â”‚   â”œâ”€â”€ dataset.py        # æ•°æ®é›†ç±»
â”‚   â””â”€â”€ preprocess.py     # æ•°æ®é¢„å¤„ç†è„šæœ¬
â””â”€â”€ save_model/           # æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼‰
    â””â”€â”€ epoch97/
        â”œâ”€â”€ config.json
        â””â”€â”€ pytorch_model.bin
```

## âš™ï¸ é…ç½®è¯´æ˜

ä¸»è¦é…ç½®åœ¨ `parameter_config.py` ä¸­ï¼Œä¸»è¦å‚æ•°è¯´æ˜ï¼š

### è®¾å¤‡é…ç½®
- `device`: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨ CPU æˆ– CUDAï¼ˆGPUï¼‰

### è·¯å¾„é…ç½®
- `vocab_path`: è¯æ±‡è¡¨è·¯å¾„ï¼ˆé»˜è®¤ `./vocab/vocab.txt`ï¼‰
- `train_path`: è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆé»˜è®¤ `data/medical_train.pkl`ï¼‰
- `valid_path`: éªŒè¯æ•°æ®è·¯å¾„ï¼ˆé»˜è®¤ `data/medical_valid.pkl`ï¼‰
- `config_json`: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ `config/config.json`ï¼‰
- `save_model_path`: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ `save_model2`ï¼‰

### ç”Ÿæˆå‚æ•°
- `max_history_len`: å†å²å¯¹è¯æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ 3ï¼‰ï¼Œæ§åˆ¶æ¨¡å‹è®°ä½å¤šå°‘è½®å†å²å¯¹è¯
- `max_len`: ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ 300ï¼‰ï¼Œè¶…è¿‡æ­¤é•¿åº¦ä¼šæˆªæ–­
- `repetition_penalty`: é‡å¤æƒ©ç½šå‚æ•°ï¼ˆé»˜è®¤ 10.0ï¼‰ï¼Œå€¼è¶Šå¤§è¶Šèƒ½å‡å°‘é‡å¤ç”Ÿæˆ
- `topk`: Top-K é‡‡æ ·å‚æ•°ï¼ˆé»˜è®¤ 4ï¼‰ï¼Œåªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªè¯ä¸­é€‰æ‹©
- `topp`: Top-P (Nucleus) é‡‡æ ·å‚æ•°ï¼ˆé»˜è®¤ 0.2ï¼‰ï¼Œç´¯ç§¯æ¦‚ç‡é˜ˆå€¼

### è®­ç»ƒå‚æ•°
- `batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ 8ï¼‰
- `epochs`: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 4ï¼‰
- `lr`: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 2.6e-5ï¼‰
- `warmup_steps`: å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼ˆé»˜è®¤ 100ï¼‰
- `max_grad_norm`: æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé»˜è®¤ 2.0ï¼‰
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆé»˜è®¤ 1ï¼‰

## ğŸ”§ æ•°æ®æ ¼å¼

### è®­ç»ƒæ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®åº”ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
ç”¨æˆ·1: ä½ å¥½
æœºå™¨äºº1: æ‚¨å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ

ç”¨æˆ·2: æˆ‘æœ€è¿‘æœ‰ç‚¹å¤´ç–¼
æœºå™¨äºº2: å¤´ç–¼çš„åŸå› æœ‰å¾ˆå¤šï¼Œå»ºè®®æ‚¨å¤šä¼‘æ¯ï¼Œå¦‚æœæŒç»­ä¸ç¼“è§£ï¼Œå»ºè®®å°±åŒ»æ£€æŸ¥ã€‚
```

- æ¯ä¸ªå¯¹è¯ç”±å¤šè½®ç»„æˆï¼Œç”¨ç©ºè¡Œåˆ†éš”ä¸åŒå¯¹è¯
- æ¯è½®å¯¹è¯æ ¼å¼ï¼š`ç”¨æˆ·: å†…å®¹` æˆ– `æœºå™¨äºº: å†…å®¹`
- æ”¯æŒ Windows (`\r\n`) å’Œ Linux (`\n`) æ¢è¡Œç¬¦

### æ•°æ®é¢„å¤„ç†

è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ ¼å¼ï¼š

```bash
cd data_preprocess
python preprocess.py
```

é¢„å¤„ç†åçš„æ•°æ®ä¼šä¿å­˜ä¸º `.pkl` æ ¼å¼ï¼ŒåŒ…å« tokenized çš„å¯¹è¯åºåˆ—ã€‚

## ğŸ“ è®­ç»ƒæ¨¡å‹

å¦‚æœéœ€è¦è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼š

### 1. å‡†å¤‡è®­ç»ƒæ•°æ®

å°†è®­ç»ƒæ•°æ®æŒ‰ç…§ä¸Šè¿°æ ¼å¼æ”¾ç½®åœ¨ `data/` ç›®å½•ä¸‹ï¼Œä¾‹å¦‚ï¼š
- `data/medical_train.txt` - è®­ç»ƒé›†
- `data/medical_valid.txt` - éªŒè¯é›†

### 2. è¿è¡Œæ•°æ®é¢„å¤„ç†

```bash
python data_preprocess/preprocess.py
```

**æ³¨æ„**ï¼šéœ€è¦ä¿®æ”¹ `preprocess.py` ä¸­çš„æ–‡ä»¶è·¯å¾„ã€‚

### 3. é…ç½®è®­ç»ƒå‚æ•°

åœ¨ `parameter_config.py` ä¸­è°ƒæ•´è®­ç»ƒå‚æ•°ï¼š
- ä¿®æ”¹ `train_path` å’Œ `valid_path` æŒ‡å‘ä½ çš„æ•°æ®æ–‡ä»¶
- æ ¹æ®ä½ çš„ç¡¬ä»¶é…ç½®è°ƒæ•´ `batch_size`
- è®¾ç½® `epochs` å’Œ `save_model_path`

### 4. å¼€å§‹è®­ç»ƒ

```bash
python train.py
```

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šï¼š
- è‡ªåŠ¨ä¿å­˜æ¯ 10 ä¸ª epoch çš„æ¨¡å‹
- ä¿å­˜éªŒè¯é›†æŸå¤±æœ€ä½çš„æ¨¡å‹åˆ° `min_ppl_model_bj/` ç›®å½•
- æ˜¾ç¤ºè®­ç»ƒæŸå¤±ã€å‡†ç¡®ç‡å’Œå­¦ä¹ ç‡ç­‰ä¿¡æ¯

### 5. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œä¿®æ”¹ `flask_predict.py` æˆ– `interact.py` ä¸­çš„ `model_path` æŒ‡å‘ä½ çš„æ¨¡å‹ç›®å½•ã€‚

## ğŸ” æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹æ¶æ„
- **åŸºç¡€æ¨¡å‹**: GPT-2 (GPT2LMHeadModel)
- **åˆ†è¯å™¨**: BERT Tokenizer (BertTokenizerFast)
- **ç‰¹æ®Šæ ‡è®°**: `[CLS]`ï¼ˆå¯¹è¯å¼€å§‹ï¼‰ã€`[SEP]`ï¼ˆåˆ†éš”ç¬¦ï¼‰ã€`[PAD]`ï¼ˆå¡«å……ï¼‰

### ç”Ÿæˆç­–ç•¥
- **Top-K é‡‡æ ·**: åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ªè¯ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªè¯
- **Top-P (Nucleus) é‡‡æ ·**: ä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„æœ€å°è¯é›†åˆä¸­é€‰æ‹©
- **é‡å¤æƒ©ç½š**: å¯¹å·²ç”Ÿæˆçš„è¯é™ä½å…¶ç”Ÿæˆæ¦‚ç‡ï¼Œå‡å°‘é‡å¤

### å¯¹è¯æ ¼å¼
æ¨¡å‹è¾“å…¥æ ¼å¼ï¼š`[CLS]utterance1[SEP]utterance2[SEP]utterance3[SEP]...`

### è®­ç»ƒä¼˜åŒ–
- **å­¦ä¹ ç‡é¢„çƒ­**: ä½¿ç”¨çº¿æ€§é¢„çƒ­ç­–ç•¥ï¼Œç¨³å®šè®­ç»ƒåˆæœŸ
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå°æ‰¹æ¬¡è®­ç»ƒï¼Œæ¨¡æ‹Ÿå¤§æ‰¹æ¬¡æ•ˆæœ

## ğŸ› å¸¸è§é—®é¢˜

### Q: è¿è¡Œæ—¶æç¤ºæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼Ÿ
A: è¯·ç¡®ä¿å·²ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶æ”¾ç½®åœ¨ `save_model/epoch97/` ç›®å½•ä¸‹ï¼Œæˆ–ä¿®æ”¹ `flask_predict.py` ä¸­çš„ `model_path` å˜é‡ã€‚

### Q: å¦‚ä½•ä¿®æ”¹æ¨¡å‹è·¯å¾„ï¼Ÿ
A: åœ¨ `flask_predict.py` ä¸­ä¿®æ”¹ `model_path` å˜é‡ï¼š
```python
model_path = os.path.join(BASE_DIR, 'save_model/epoch97')
```

### Q: å¦‚ä½•ä½¿ç”¨ GPUï¼Ÿ
A: 
1. ç¡®ä¿å®‰è£…äº† CUDA ç‰ˆæœ¬çš„ PyTorch
2. ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
3. å¯ä»¥é€šè¿‡ `os.environ["CUDA_VISIBLE_DEVICES"] = '0'` æŒ‡å®šä½¿ç”¨çš„ GPU

### Q: ç”Ÿæˆçš„å›å¤è´¨é‡ä¸ä½³ï¼Ÿ
A: å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
- å¢åŠ  `repetition_penalty` å‡å°‘é‡å¤ï¼ˆå¦‚æ”¹ä¸º 15.0ï¼‰
- è°ƒæ•´ `topk`ï¼ˆå¦‚æ”¹ä¸º 8ï¼‰å’Œ `topp`ï¼ˆå¦‚æ”¹ä¸º 0.9ï¼‰å‚æ•°
- å¢åŠ  `max_history_len` è®©æ¨¡å‹è®°ä½æ›´å¤šä¸Šä¸‹æ–‡
- ä½¿ç”¨æ›´å¥½çš„è®­ç»ƒæ•°æ®é‡æ–°è®­ç»ƒæ¨¡å‹
- å¢åŠ è®­ç»ƒè½®æ•° `epochs`

### Q: å†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: 
- å‡å° `batch_size`
- å‡å° `max_len` å’Œ `max_history_len`
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ (`gradient_accumulation_steps`)
- ä½¿ç”¨ CPU è®­ç»ƒï¼ˆè™½ç„¶é€Ÿåº¦è¾ƒæ…¢ï¼‰

### Q: è®­ç»ƒé€Ÿåº¦å¾ˆæ…¢ï¼Ÿ
A: 
- ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆå®‰è£… CUDA ç‰ˆæœ¬çš„ PyTorchï¼‰
- å¢åŠ  `batch_size`ï¼ˆå¦‚æœå†…å­˜å…è®¸ï¼‰
- å‡å°‘ `gradient_accumulation_steps`
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è¦é¢å¤–é…ç½®ï¼‰

### Q: å¦‚ä½•ä¿®æ”¹ Web ç•Œé¢ç«¯å£ï¼Ÿ
A: åœ¨ `app.py` ä¸­ä¿®æ”¹ï¼š
```python
app.run(debug=True, port=5000)  # ä¿®æ”¹ port å‚æ•°
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **æ¨ç†ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨ GPU è¿›è¡Œæ¨ç†
   - æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚ï¼ˆéœ€è¦ä¿®æ”¹ä»£ç ï¼‰
   - ä½¿ç”¨æ¨¡å‹é‡åŒ–å‡å°‘å†…å­˜å ç”¨

2. **è®­ç»ƒä¼˜åŒ–**ï¼š
   - ä½¿ç”¨å¤š GPU è®­ç»ƒï¼ˆä¿®æ”¹ `CUDA_VISIBLE_DEVICES`ï¼‰
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
   - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜

3. **æ•°æ®ä¼˜åŒ–**ï¼š
   - é¢„å¤„ç†æ•°æ®å¹¶ä¿å­˜ä¸º `.pkl` æ ¼å¼
   - ä½¿ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½ï¼ˆä¿®æ”¹ `dataloader.py`ï¼‰

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒ Web ç•Œé¢å’Œå‘½ä»¤è¡Œäº¤äº’
- æ”¯æŒæ¨¡å‹è®­ç»ƒå’Œæ¨ç†
- å®ç°å¤šè½®å¯¹è¯åŠŸèƒ½

## ğŸ“„ è®¸å¯è¯

[æ·»åŠ ä½ çš„è®¸å¯è¯ä¿¡æ¯]

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

è´¡çŒ®æŒ‡å—ï¼š
1. Fork æœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“® è”ç³»æ–¹å¼

[æ— ]

## ğŸ™ è‡´è°¢

- [Hugging Face Transformers](https://github.com/huggingface/transformers) - æä¾› GPT-2 æ¨¡å‹å®ç°
- [Flask](https://flask.palletsprojects.com/) - Web æ¡†æ¶
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
